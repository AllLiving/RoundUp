



MapReduce常用组件介绍

### HadoopMapReduce jobs

- 可以切分成一系列运行于分布式集群中的map和reduce任务；
- 每个任务只运行全部数据的一个指定的子集，以此达到整个集群的负载平衡；

### Map和Reduce

通常为加载，解析，转换，过滤数据，每个reduce处理map输出的一个子集。Reduce任务会去map任务端copy中间数据来完成分组，聚合。

### MapReduce 

MapReduce 的输入是hdfs上存储的一系列文件集。

1. 每个map任务被分为以下阶段：record reader，mapper，combiner，partitioner。

   Map任务的输出叫中间数据，包括keys和values，发送到reduce端。

   运行map任务的节点会尽量选择数据所在节点。

2. Combiner 是一个map阶段分组数据，可选的，局部reducer。它根据用户提供的方法在一个mapper范围内根据中间键去聚合值。Partitioner会获取从mapper（或combiner）来的键值对，并分割成分片，每个reducer一个分片。默认用哈希值，典型使用md5sum。

3. 然后partitioner根据reduce的个数执行取余运算：key.hashCode() % (number of reducers)。这样能随机均匀的根据key分发数据到reduce，同时保证不同mapper的相同key分到同一个reduce。Partitioner也可以自定义，使用更高级的样式，例如排序。然而，更改partitioner很少用。Partitioner的每个map的数据会写到本地磁盘，并等待对应的reducer检测，拿走数据。       

4. Reduce任务分为以下阶段：shuffle，sort，reduce，output format。

   Reduce任务开始于shuffle和sort阶段。这一阶段获取partitioner的输出文件，并下载到reduce运行的本地机器。这些分片数据会根据key合并，排序成一个大的数据文件。排序的目的是让相同的key相邻，方便在reduce阶段值得迭代处理。这一阶段不能自定义，由框架自动处理。需要做的只是key的选择和可以自定义用于分组的比较器。